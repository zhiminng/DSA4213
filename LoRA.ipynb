{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-IhTHQsRlOU"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets peft accelerate evaluate torch bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN00W1e8U2wG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5aWD6XngVel2",
    "outputId": "92a4d1ac-1289-42f7-ba7b-5875210188fb"
   },
   "outputs": [],
   "source": [
    "file_path = r\"/content/Sentences_66Agree.txt\"  # <-- Update path if needed\n",
    "\n",
    "# Use cp1252 encoding\n",
    "with open(file_path, 'r', encoding='cp1252') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "    if \"@positive\" in line:\n",
    "        label = \"positive\"\n",
    "        text = line.replace(\"@positive\", \"\").strip()\n",
    "    elif \"@negative\" in line:\n",
    "        label = \"negative\"\n",
    "        text = line.replace(\"@negative\", \"\").strip()\n",
    "    elif \"@neutral\" in line:\n",
    "        label = \"neutral\"\n",
    "        text = line.replace(\"@neutral\", \"\").strip()\n",
    "    else:\n",
    "        continue  # skip malformed lines\n",
    "\n",
    "    sentences.append(text)\n",
    "    labels.append(label)\n",
    "\n",
    "print(f\"âœ… Loaded {len(sentences)} sentences.\")\n",
    "\n",
    "# Map labels to integers\n",
    "df = pd.DataFrame({\"text\": sentences, \"label\": labels})\n",
    "df[\"label\"] = df[\"label\"].map({\"negative\": 0, \"neutral\": 1, \"positive\": 2})\n",
    "\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yHaUpnaVfj6"
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42, stratify=test_df[\"label\"])\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "a3cf25b695dd486388cfe7b886a7b2f6",
      "99bd8b038ad249b98762ce7a63bfe172",
      "d1a5194514644e9b9f29665b6a68429d",
      "e8ec205304e846e198d8ffd08c1b9a14",
      "debd3eaf3df549e2b703321780fa1320",
      "dc3b1e16e63e4539b28e6a64d1befff7",
      "550f5b932b8c43d4b4882e2a1c712dea",
      "a9443771539d41f2baf221b1b579c0c2",
      "6e4060292ac94576ae100b718501cabd",
      "b0bc838da01a41de97039df9250cf97e",
      "8b4bbccbbede44bf90a110d99139e52f",
      "32f769b3eb7e431ba6ed8dbf581f91d4",
      "2f47275448bc4368b5ac1cca273d9f36",
      "67b9ee860c484c45bf17cc6847717ee6",
      "28e8d670afe545a4b3534d0b9c0dbd19",
      "a939d57e442c401b9cfaa7a0fc943d23",
      "ac47869bfa7549578a49441588642829",
      "b24bb1e6b1bd48adb854289576ba68a5",
      "b557f35a8d6f416292695cf79ed27cd6",
      "7850cd4126e74c7a93f598d8f84eee0f",
      "8c863c38db5043669ae4729a6170c6d8",
      "02e340b9764746bab29e3880f06bfc5e",
      "b25d102202694063a65a3d65810821dd",
      "7b12be197dc947768794aa97fc0221aa",
      "09904ac1f3c24635a2a6ffb799e770b4",
      "af6487e3b8754e4b8bd597947ca2cdbe",
      "a72bdbef1b4847689227d44c2ed84c58",
      "c87336a5f0474d178a33f271f908e83d",
      "af6bcd473599427ea4459542a5778bf6",
      "65886d4caf65446ab662226446ad1895",
      "d6dc7975a8b84554916cba088fd79240",
      "157089dead774173b05f6c19337cb159",
      "1b4f2d50339248be8704b4a49e5bff82"
     ]
    },
    "id": "nAvw5bghVjuB",
    "outputId": "f557df5a-4837-48f4-c083-01b3362171d0"
   },
   "outputs": [],
   "source": [
    "# 2ï¸âƒ£ Tokenization\n",
    "# ======================================\n",
    "model_name = \"yiyanghkust/finbert-tone\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqnTM5xSagVJ"
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"yiyanghkust/finbert-tone\",\n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNGXMUP7Vs59",
    "outputId": "1746a3ce-bff3-4086-e702-c46d57756fb5"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                      # LoRA rank\n",
    "    lora_alpha=32,            # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"],  # Apply LoRA to attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekHj3ec_VwA-"
   },
   "outputs": [],
   "source": [
    "# 4ï¸âƒ£ Define metrics\n",
    "# ======================================\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"f1_negative\": f1_per_class[0],\n",
    "        \"f1_neutral\": f1_per_class[1],\n",
    "        \"f1_positive\": f1_per_class[2],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4_yPnfQV2D1"
   },
   "outputs": [],
   "source": [
    "# 5ï¸âƒ£ Training arguments\n",
    "# ======================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./news_classifier_lora\",\n",
    "    learning_rate=5e-5,                 # slightly higher than full FT\n",
    "    per_device_train_batch_size=32,     # same as full FT\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,                 # same as full FT\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_lora\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=True,                          # same as full FT if using GPU\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rBalWF8V7zk",
    "outputId": "d451c493-10a5-4a5a-8b95-6d35221bfa5e"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# 7ï¸âƒ£ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "re9Dqnh5V_Ub",
    "outputId": "c0519303-16cd-48c8-f4cd-26ca4869d303"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "0PHgYxmHWB_F",
    "outputId": "b76b5903-2bbb-45f7-ca00-e703ef946851"
   },
   "outputs": [],
   "source": [
    "# 9ï¸âƒ£ Evaluate LoRA model\n",
    "# ======================================\n",
    "eval_results_lora = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "for key, value in eval_results_lora.items():\n",
    "    if key.startswith('eval_'):\n",
    "        metric_name = key.replace('eval_', '').upper()\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "predictions_lora = trainer.predict(test_dataset)\n",
    "y_pred_lora = np.argmax(predictions_lora.predictions, axis=-1)\n",
    "y_true = predictions_lora.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9X_LOFyWEVn",
    "outputId": "58648585-bd16-495d-a154-88e47be91a73"
   },
   "outputs": [],
   "source": [
    "# ðŸ”Ÿ Save LoRA adapter (tiny size)\n",
    "# ======================================\n",
    "print(\"\\nðŸ’¾ Saving LoRA model...\")\n",
    "model.save_pretrained(\"./news_classifier_lora\")\n",
    "tokenizer.save_pretrained(\"./news_classifier_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695
    },
    "id": "I91DfbJBaSTG",
    "outputId": "e9afd1f2-52d2-4235-d9e6-31af8eb101ad"
   },
   "outputs": [],
   "source": [
    "# ðŸ”¢ Reports\n",
    "# ======================================\n",
    "print(\"\\nðŸ“Š Classification Report (LoRA):\")\n",
    "print(classification_report(y_true, y_pred_lora, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred_lora)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "            yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - LoRA Fine-tuning\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "M65NdCgNKbvu",
    "outputId": "fedac5e3-5c8a-462d-a0fc-ab4ddfeaf3ad"
   },
   "outputs": [],
   "source": [
    "metrics_df_lora = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# 2ï¸âƒ£ Separate training and evaluation logs\n",
    "train_loss_df = metrics_df_lora[metrics_df_lora[\"loss\"].notnull()]\n",
    "eval_loss_df = metrics_df_lora[metrics_df_lora[\"eval_loss\"].notnull()]\n",
    "\n",
    "# 3ï¸âƒ£ Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss_df[\"epoch\"], train_loss_df[\"loss\"], 'o-', label=\"Training Loss\")\n",
    "plt.plot(eval_loss_df[\"epoch\"], eval_loss_df[\"eval_loss\"], 's-', label=\"Validation Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss - LoRA\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
